{"cells":[{"cell_type":"markdown","metadata":{"id":"tjEdUh94Gqje"},"source":["# **Embedded AI workshop**\n","## **Static Quantization**\n","### *Mohammmad Ali Zamani*\n","\n","*Senior Machine Learning Scientist at HITeC e.V.*\n","\n","homepage: [zamani.ai](https://zamani.ai/)\n"]},{"cell_type":"markdown","metadata":{"id":"uZ3PCtZwb9DW"},"source":["\n","\n","In this tutorial, you will learn how to use `torch.ao.quantization` package to\n","quantize your neural networks.\n","\n","For more information:\n","\n","[1- Pytorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n","\n","[2- Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Setup\n","======\n","we installing some packages and import the necessary libraries."],"metadata":{"id":"YAAvw9p3CSW_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJApWUhpBCO"},"outputs":[],"source":["!pip install torch-pruning > /dev/null 2>&1\n","!pip install -q gwpy > /dev/null 2>&1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FMdogqvb9Dc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import os\n","import torch_pruning as tp\n","from torch.ao.quantization import QuantStub, DeQuantStub\n","from torch.ao.quantization import QConfig\n","from torch.ao.quantization.observer import HistogramObserver, PerChannelMinMaxObserver\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"9Ve76Yvxb9De"},"source":["Creating a model, transform, and dataloader\n","==============\n","\n","In this tutorial, we use the\n","[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture\n","from LeCun et al., 1998.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C84tDHaSb9De"},"outputs":[],"source":["# select the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.quant = QuantStub() # new module that converts tensors from floating point to quantized\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.relu2 = nn.ReLU()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(50, 10)\n","        self.dequant = DeQuantStub() # new module that converts tensors from quantized to floating point\n","\n","    def forward(self, x):\n","        # manually specify where tensors will be converted from floating\n","        # point to quantized in the quantized model\n","        x = self.quant(x)\n","\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = x.reshape(-1, int(x.nelement() / x.shape[0]))\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        # manually specify where tensors will be converted from quantized\n","        # to floating point in the quantized model\n","        x = self.dequant(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"YrhugF_Cb7ON"},"source":["we create transforms with usual data augmentation suitable for the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IrcBc0-b-A7"},"outputs":[],"source":["# Transform for MNIST data\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(degrees=20),\n","    transforms.RandomInvert(p=0.5),\n","    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),\n","    transforms.Normalize((0.5,), (0.5,)),\n","    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n","    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","    # transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])"]},{"cell_type":"markdown","metadata":{"id":"xDffhsz2cWtN"},"source":["We use the following lines to download the dataset and create the data loader."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXbs2ef1cXGa"},"outputs":[],"source":["# for hiding the output of the download\n","%%capture\n","\n","# Load MNIST training and test sets\n","trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"34KNVt4Mdf1I"},"source":["Visualizing and repots\n","====\n","We selected some images from the test set for visualizing and evaluating the prediction. The `get_model_size` function measures how much space the model needs. The `print_model_test_report` function creates a report for the actual network architecture, measurements, accuracy, and sample images."]},{"cell_type":"code","source":["# Define the indices for the demo images\n","demo_indices = [3, 2, 1, 18, 4, 23, 11, 17, 61, 9]\n","\n","# Demo inputs and corresponding digits\n","demo_inputs = torch.vstack([testset[i][0] for i in demo_indices]).unsqueeze(1).to(device=device)\n","demos = [(i, index) for i, index in enumerate(demo_indices)]\n","\n","def demo_image(digit):\n","    return demo_inputs[digit].squeeze(0).cpu().numpy()\n","\n","def visualize(model, device):\n","    plt.figure(figsize=(15,6))\n","    predictions = model(demo_inputs.to(device)).argmax(dim=1)\n","    for digit, index in demos:\n","        plt.subplot(1, 10, digit + 1)\n","        plt.imshow(demo_image(digit))\n","        plt.title(f\"digit: {digit}\\npred: {int(predictions[digit])}\")\n","        plt.axis('off')\n","    plt.show()"],"metadata":{"id":"58WC8fKu8-po"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sBHSO1Wdl4z"},"outputs":[],"source":["def get_model_size(model):\n","    torch.save(model.state_dict(), \"tmp.pt\")\n","    model_size = os.path.getsize(\"tmp.pt\")/1e3\n","    os.remove('tmp.pt')\n","    return model_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0f00LBY4baV"},"outputs":[],"source":["# a pretty print to evaluate the model after each training\n","def print_model_test_report(msg, model, example_inputs, testloader, device='cuda'):\n","    acc = evaluate_model(model, testloader, device)\n","    print(\"\\n%s:\\n\" % (msg))\n","    print(model)\n","    print(\"\\nSize: %.2f KB,  Test_Acc:  %.2f%% \\n\" % (get_model_size(model), acc))\n","    visualize(model, device=device)\n"]},{"cell_type":"markdown","metadata":{"id":"hBBUqcTOd2Fd"},"source":["Training and evaluating\n","====\n","The `evaluate_model` can evaluate the testset. The `training_loop` is the main training loop of pytorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTuvx1t5dgLv"},"outputs":[],"source":["def evaluate_model(model, testloader, device=\"cuda\"):\n","    # Test the network on the test data\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n","        for data in testloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    acc = 100 * correct / total\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEPHPsf9d8xk"},"outputs":[],"source":["def training_loop(model, trainloader, iter):\n","    # Train the network\n","    for epoch in range(1):  # increase it for loop over the dataset multiple times\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Initialize tqdm progress bar\n","        progress_bar = tqdm(enumerate(trainloader, 0), total=len(trainloader), desc=f\"training Epoch {epoch+1}\")\n","\n","\n","        for i, data in progress_bar:\n","\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 10 == 9:    # print every 10 mini-batches\n","                progress_bar.set_postfix(loss=running_loss / 10)\n","                running_loss = 0.0\n","\n","        progress_bar.close()\n"]},{"cell_type":"markdown","metadata":{"id":"JGQNTypYeMdF"},"source":["Creating model, optimizer and loss function\n","====\n","In the following cell, the model, optimizer and loss function are created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsBz_BtBezpv"},"outputs":[],"source":["# create a model instance\n","model_fp32 = Net().to(device=device)\n","\n","# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_fp32.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","source":["Quantization\n","====\n"],"metadata":{"id":"SQ9AqLr0Hvzn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKc1H4vgeM0D"},"outputs":[],"source":["example_inputs = torch.randn(1, 1, 28, 28).to(device=device)\n","training_loop(model_fp32, trainloader, 0)\n","print_model_test_report(\"before quantization\", model_fp32, example_inputs, testloader, device=device)\n","\n","# separate after quantization with one line\n","print(\"\\n\"+ \"=\"*160 + \"\\n\")\n","\n","# model must be set to eval for fusion to work\n","model_fp32.eval()\n","\n","# Define the new QConfig with quant_min and quant_max\n","activation_observer = HistogramObserver.with_args(quant_min=0, quant_max=127)\n","weight_observer = PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_affine)\n","qconfig = QConfig(activation=activation_observer, weight=weight_observer)\n","model_fp32.qconfig = qconfig\n","\n","\n","# Fuse the activations to preceding layers, where applicable.\n","# This needs to be done manually depending on the model architecture.\n","# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n","model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv1', 'relu1'], ['conv2', 'relu2']])\n","\n","# Prepare the model for static quantization. This inserts observers in\n","# the model that will observe activation tensors during calibration.\n","model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n","\n","# Run calibration data through the model.\n","# with torch.no_grad():\n","#     for input_fp32 , _ in trainloader:  # Use your trainloader or a dedicated calibration DataLoader\n","#         input_fp32  = input_fp32.to(device)\n","#         model_fp32_prepared(input_fp32 )\n","\n","# Convert the observed model to a quantized model. This does several things:\n","# quantizes the weights, computes and stores the scale and bias value to be\n","# used with each activation tensor, and replaces key operators with quantized\n","# implementations.\n","model_int8 = torch.ao.quantization.convert(model_fp32_prepared.to('cpu'))\n","\n","print_model_test_report(\"after quantization\", model_int8, example_inputs, testloader, device='cpu')\n","\n","# Assignment:\n","#\n","# 1- uncomment the calibration part, compare accuracy, scales, and zeros before and after calibration\n","# 2- try different quantization scheme (qscheme):\n","#   e.g., torch.per_channel_affine, per_channel_symmetric\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb","timestamp":1715696210218}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}