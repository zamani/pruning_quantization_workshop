{"cells":[{"cell_type":"markdown","metadata":{"id":"tjEdUh94Gqje"},"source":["# **Embedded AI workshop**\n","## **Pruning and Finetuning**\n","### *Mohammmad Ali Zamani*\n","\n","*Senior Machine Learning Scientist at HITeC e.V.*\n","\n","homepage: [zamani.ai](https://zamani.ai/)\n"]},{"cell_type":"markdown","metadata":{"id":"uZ3PCtZwb9DW"},"source":["\n","\n","In this tutorial, you will learn how to use `torch_pruning` package to\n","sparsify your neural networks.\n","\n","For more information:\n","\n","[1- Pytorch tutorial for pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n","\n","[2- Torch Pruning](https://github.com/VainF/Torch-Pruning)\n","\n"]},{"cell_type":"markdown","source":["Setup\n","======\n","we installing some packages and import the necessary libraries."],"metadata":{"id":"YAAvw9p3CSW_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJApWUhpBCO"},"outputs":[],"source":["!pip install torch-pruning > /dev/null 2>&1\n","!pip install -q gwpy > /dev/null 2>&1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FMdogqvb9Dc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch_pruning as tp\n","import torch.optim as optim\n","import os\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"9Ve76Yvxb9De"},"source":["Creating a model, transform, and dataloader\n","==============\n","\n","In this tutorial, we use the\n","[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture\n","from LeCun et al., 1998.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C84tDHaSb9De"},"outputs":[],"source":["# select the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = x.view(-1,  int(x.nelement() / x.shape[0]))\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"YrhugF_Cb7ON"},"source":["we create transforms with usual data augmentation suitable for the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IrcBc0-b-A7"},"outputs":[],"source":["# Transform for MNIST data\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(degrees=20),\n","    transforms.RandomInvert(p=0.5),\n","    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),\n","    transforms.Normalize((0.5,), (0.5,)),\n","    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n","    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","    # transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])"]},{"cell_type":"markdown","metadata":{"id":"xDffhsz2cWtN"},"source":["We use the following lines to download the dataset and create the data loader."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXbs2ef1cXGa"},"outputs":[],"source":["# for hiding the output of the download\n","%%capture\n","\n","# Load MNIST training and test sets\n","trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"34KNVt4Mdf1I"},"source":["Visualizing and repots\n","====\n","We selected some images from the test set for visualizing and evaluating the prediction. The `get_model_size` function measures how much space the model needs. The `print_model_test_report` function creates a report for the actual network architecture, measurements, accuracy, and sample images."]},{"cell_type":"code","source":["# Define the indices for the demo images\n","demo_indices = [3, 2, 1, 18, 4, 23, 11, 17, 61, 9]\n","\n","# Demo inputs and corresponding digits\n","demo_inputs = torch.vstack([testset[i][0] for i in demo_indices]).unsqueeze(1).to(device=device)\n","demos = [(i, index) for i, index in enumerate(demo_indices)]\n","\n","def demo_image(digit):\n","    return demo_inputs[digit].squeeze(0).cpu()\n","\n","def visualize(model, with_prediction=False):\n","    plt.figure(figsize=(15,6))\n","    predictions = model(demo_inputs).argmax(dim=1) if with_prediction else None\n","    for digit, index in demos:\n","        plt.subplot(1, 10, digit + 1)\n","        plt.imshow(demo_image(digit))\n","        if predictions is None:\n","            plt.title(f\"digit: {digit}\")\n","        else:\n","            plt.title(f\"digit: {digit}\\npred: {int(predictions[digit])}\")\n","        plt.axis('off')\n","    plt.show()"],"metadata":{"id":"58WC8fKu8-po"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sBHSO1Wdl4z"},"outputs":[],"source":["def get_model_size(model):\n","    torch.save(model.state_dict(), \"tmp.pt\")\n","    model_size = os.path.getsize(\"tmp.pt\")/1e3\n","    os.remove('tmp.pt')\n","    return model_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0f00LBY4baV"},"outputs":[],"source":["# a pretty print to evaluate the model after each training\n","def print_model_test_report(msg, model, example_inputs, testloader):\n","    acc = evaluate_model(model, testloader)\n","    print(\"\\n%s:\\n\" % (msg))\n","    print(model)\n","    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n","    print(\"\\n#Params: %.2f K,  #MACs: %.2f M,  Size: %.2f KB,  Test_Acc:  %.2f%% \\n\" % (nparams / 1e3, macs / 1e6, get_model_size(model), acc))\n","    visualize(model, with_prediction=True)\n"]},{"cell_type":"markdown","metadata":{"id":"hBBUqcTOd2Fd"},"source":["Training and evaluating\n","====\n","The `evaluate_model` can evaluate the testset. The `training_loop` is the main training loop of pytorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTuvx1t5dgLv"},"outputs":[],"source":["def evaluate_model(model, testloader):\n","    # Test the network on the test data\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n","        for data in testloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    acc = 100 * correct / total\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEPHPsf9d8xk"},"outputs":[],"source":["def training_loop(model, trainloader, iter):\n","    # Train the network\n","    for epoch in range(1):  # increase it for loop over the dataset multiple times\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Initialize tqdm progress bar\n","        progress_bar = tqdm(enumerate(trainloader, 0), total=len(trainloader), desc=f\"pruning iter {iter} -> training Epoch {epoch+1}\")\n","\n","\n","        for i, data in progress_bar:\n","\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 10 == 9:    # print every 10 mini-batches\n","                progress_bar.set_postfix(loss=running_loss / 10)\n","                running_loss = 0.0\n","\n","        progress_bar.close()\n"]},{"cell_type":"markdown","metadata":{"id":"JGQNTypYeMdF"},"source":["Creating model, optimizer and loss function\n","====\n","In the following cell, the model, optimizer and loss function are created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsBz_BtBezpv"},"outputs":[],"source":["model = Net().to(device=device)\n","\n","# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","source":["Pruner Setup\n","====\n"],"metadata":{"id":"SQ9AqLr0Hvzn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKc1H4vgeM0D"},"outputs":[],"source":["# 1. create an example input\n","example_inputs = torch.randn(1, 1, 28, 28).to(device=device)\n","\n","# 2. importance criterion\n","imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')\n","\n","# 3. ignore some layers that should not be pruned, e.g., the final classifier layer.\n","ignored_layers = []\n","for m in model.modules():\n","    if isinstance(m, torch.nn.Linear) and  m.out_features == 10:\n","        ignored_layers.append(m) # DO NOT prune the final classifier!\n","\n","# 4. decide the pruning iterations and pruning ratio\n","iterative_steps = 1 # in this code, if iterative_steps=1, it means the model is trained, and then pruned without further finetuning\n","pruning_ratio = 0.8 # e.g. 0.8 means 80% of the network will be pruned.\n","\n","# 5. Pruner initialization\n","pruner_iterative = tp.pruner.MetaPruner(\n","    model,\n","    example_inputs,\n","    global_pruning=False, # If False, a uniform pruning ratio will be assigned to different layers.\n","    importance=imp, # importance criterion for parameter selection\n","    iterative_steps=iterative_steps, # the number of iterations to achieve target pruning ratio\n","    pruning_ratio=pruning_ratio, # remove n% channels and nodes iteratively\n","    # pruning_ratio_dict = {model.conv1: 0.2, model.layer2: 0.8}, # customized pruning ratios for layers or blocks\n","    ignored_layers=ignored_layers,\n",")"]},{"cell_type":"markdown","metadata":{"id":"_EP5aJGe3OXT"},"source":["# Pruning and Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCklrMu2ek-3"},"outputs":[],"source":["# pruning + finetuning\n","for i in range(iterative_steps):\n","\n","    # train the model\n","    training_loop(model, trainloader, i)\n","\n","    # evaluate the model accuracy and measurements such as number of parameters, MAC, and Size\n","    print_model_test_report(\"result\", model, example_inputs, testloader)\n","\n","    # perform one step of pruning\n","    pruner_iterative.step()\n","\n","    # separate the next iteration of pruning with one line\n","    print(\"\\n\"+ \"=\"*160 + \"\\n\")\n","\n","# evaluate the model after the final iteration of the pruning\n","print_model_test_report(\"final result after \" + str(i + 1) +  \"times pruning\", model, example_inputs, testloader)\n","\n","# assignment:\n","# If iterative_steps=1, it means there is no finetuning. So, you can change the iterative_steps (e.g., to 5) to finetune the model after each pruning"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb","timestamp":1715696210218}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}