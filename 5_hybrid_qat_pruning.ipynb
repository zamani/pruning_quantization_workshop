{"cells":[{"cell_type":"markdown","metadata":{"id":"tjEdUh94Gqje"},"source":["# **Embedded AI workshop**\n","## **Quantization Aware Training and Pruning**\n","### *Mohammmad Ali Zamani*\n","\n","*Senior Machine Learning Scientist at HITeC e.V.*\n","\n","homepage: [zamani.ai](https://zamani.ai/)\n"]},{"cell_type":"markdown","metadata":{"id":"uZ3PCtZwb9DW"},"source":["\n","\n","In this tutorial, you will learn how to use `torch.ao.quantization` and `torch_pruning ` package to\n","sparsify your neural networks.\n","\n","For more information:\n","\n","[1- Pytorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n","\n","[2- Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)\n","\n","[3- Pytorch tutorial for pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n","\n","[4- Torch Pruning](https://github.com/VainF/Torch-Pruning)\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Setup\n","======\n","we installing some packages and import the necessary libraries."],"metadata":{"id":"YAAvw9p3CSW_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJApWUhpBCO"},"outputs":[],"source":["!pip install torch-pruning > /dev/null 2>&1\n","!pip install -q gwpy > /dev/null 2>&1\n","!pip install onnx > /dev/null 2>&1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FMdogqvb9Dc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import os\n","import torch_pruning as tp\n","from torch.ao.quantization import QuantStub, DeQuantStub\n","from torch.ao.quantization import QConfig\n","from torch.ao.quantization.observer import HistogramObserver, PerChannelMinMaxObserver\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"9Ve76Yvxb9De"},"source":["Creating a model, transform, and dataloader\n","==============\n","\n","In this tutorial, we use the\n","[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture\n","from LeCun et al., 1998.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C84tDHaSb9De"},"outputs":[],"source":["# select the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.quant = QuantStub()  # new module for quantization\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.relu2 = nn.ReLU()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.relu3 = nn.ReLU()\n","        self.fc2 = nn.Linear(50, 10)\n","        self.dequant = DeQuantStub() # new module for de-quantization\n","\n","    def forward(self, x):\n","        x = self.quant(x)\n","        x = F.max_pool2d(self.relu1(self.conv1(x)), 2)\n","        x = F.max_pool2d(self.relu2(self.conv2(x)), 2)\n","        x = x.reshape(-1, int(x.nelement() / x.shape[0]))\n","        x = self.relu3(self.fc1(x))\n","        x = self.fc2(x)\n","        x = self.dequant(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"YrhugF_Cb7ON"},"source":["we create transforms with usual data augmentation suitable for the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IrcBc0-b-A7"},"outputs":[],"source":["# Transform for MNIST data\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(degrees=20),\n","    transforms.RandomInvert(p=0.5),\n","    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),\n","    transforms.Normalize((0.5,), (0.5,)),\n","    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n","    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","    # transforms.RandomResizedCrop(size=(28, 28), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])"]},{"cell_type":"markdown","metadata":{"id":"xDffhsz2cWtN"},"source":["We use the following lines to download the dataset and create the data loader."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXbs2ef1cXGa"},"outputs":[],"source":["# for hiding the output of the download\n","%%capture\n","\n","# Load MNIST training and test sets\n","trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","\n","testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"34KNVt4Mdf1I"},"source":["Visualizing and repots\n","====\n","We selected some images from the test set for visualizing and evaluating the prediction. The `get_model_size` function measures how much space the model needs. The `print_model_test_report` function creates a report for the actual network architecture, measurements, accuracy, and sample images."]},{"cell_type":"code","source":["# Define the indices for the demo images\n","demo_indices = [3, 2, 1, 18, 4, 23, 11, 17, 61, 9]\n","\n","# Demo inputs and corresponding digits\n","demo_inputs = torch.vstack([testset[i][0] for i in demo_indices]).unsqueeze(1).to(device)\n","demos = [(i, index) for i, index in enumerate(demo_indices)]\n","\n","def demo_image(digit):\n","    return demo_inputs[digit].squeeze(0).cpu().numpy()\n","\n","def visualize(model, device):\n","    plt.figure(figsize=(15,6))\n","    predictions = model(demo_inputs.to(device)).argmax(dim=1)\n","    for digit, index in demos:\n","        plt.subplot(1, 10, digit + 1)\n","        plt.imshow(demo_image(digit))\n","        plt.title(f\"digit: {digit}\\npred: {int(predictions[digit])}\")\n","        plt.axis('off')\n","    plt.show()"],"metadata":{"id":"58WC8fKu8-po"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sBHSO1Wdl4z"},"outputs":[],"source":["def get_model_size(model):\n","    torch.save(model.state_dict(), \"tmp.pt\")\n","    model_size = os.path.getsize(\"tmp.pt\")/1e3\n","    os.remove('tmp.pt')\n","    return model_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0f00LBY4baV"},"outputs":[],"source":["# a pretty print to evaluate the model after each training\n","def print_model_test_report(msg, model, example_inputs, testloader, device='cuda'):\n","    acc = evaluate_model(model, testloader, device)\n","    print(\"\\n%s:\\n\" % (msg))\n","    print(model)\n","    print(\"\\nSize: %.2f KB,  Test_Acc:  %.2f%% \\n\" % (get_model_size(model), acc))\n","    visualize(model, device=device)\n"]},{"cell_type":"markdown","metadata":{"id":"hBBUqcTOd2Fd"},"source":["Training and evaluating\n","====\n","The `evaluate_model` can evaluate the testset. The `training_loop` is the main training loop of pytorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTuvx1t5dgLv"},"outputs":[],"source":["def evaluate_model(model, testloader, device=\"cuda\"):\n","    # Test the network on the test data\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n","        for data in testloader:\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    acc = 100 * correct / total\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEPHPsf9d8xk"},"outputs":[],"source":["def training_loop(model, trainloader, iter, dubg=False):\n","    # Train the network\n","    for epoch in range(1):  # increase it for loop over the dataset multiple times\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Initialize tqdm progress bar\n","        progress_bar = tqdm(enumerate(trainloader, 0), total=len(trainloader), desc=f\"training Epoch {iter+1}\")\n","\n","\n","        for i, data in progress_bar:\n","\n","            inputs, labels = data\n","\n","            if dubg and i == 0:\n","                # Open a file in write mode\n","                with open('output.txt', 'w') as f:\n","                    f.write(\"labels:\\n\")\n","                    f.write(f\"{labels.tolist()}\\n\")  # Convert the tensor to a list and write\n","                    f.write(\"inputs:\\n\")\n","\n","                    # Handle the inputs, checking if they are nested\n","                    inputs_flat = inputs[0].flatten().tolist()  # Flatten the input tensor to handle multi-dimensional cases\n","                    f.write(f\"[{', '.join(f'{float(input_value):.2f}' for input_value in inputs_flat)}]\\n\")\n","\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n","\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 10 == 9:    # print every 10 mini-batches\n","                progress_bar.set_postfix(loss=running_loss / 10)\n","                running_loss = 0.0\n","\n","        progress_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"JGQNTypYeMdF"},"source":["Creating model, optimizer and loss function\n","====\n","In the following cell, the model, optimizer and loss function are created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsBz_BtBezpv"},"outputs":[],"source":["# create a model instance\n","model = Net().to(device=device)\n","\n","# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","source":["Pruner Setup\n","====\n"],"metadata":{"id":"OgmfnedCWE7g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2t8GqDVWE7n"},"outputs":[],"source":["# 1. create an example input\n","example_inputs = torch.randn(1, 1, 28, 28).to(device=device)\n","\n","# 2. importance criterion\n","imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')\n","\n","# 3. ignore some layers that should not be pruned, e.g., the final classifier layer.\n","ignored_layers = []\n","for m in model.modules():\n","    if isinstance(m, torch.nn.Linear) and  m.out_features == 10:\n","        ignored_layers.append(m) # DO NOT prune the final classifier!\n","\n","# 4. decide the pruning iterations and pruning ratio\n","iterative_steps = 1 # in this code, if iterative_steps=1, it means the model is trained, and then pruned without further finetuning\n","pruning_ratio = 0.5 # e.g. 0.8 means 80% of the network will be pruned.\n","\n","# 5. Pruner initialization\n","pruner_iterative = tp.pruner.MetaPruner(\n","    model,\n","    example_inputs,\n","    global_pruning=False, # If False, a uniform pruning ratio will be assigned to different layers.\n","    importance=imp, # importance criterion for parameter selection\n","    iterative_steps=iterative_steps, # the number of iterations to achieve target pruning ratio\n","    pruning_ratio=pruning_ratio, # remove n% channels and nodes iteratively\n","    # pruning_ratio_dict = {model.conv1: 0.4, model.conv2: 0.4, model.fc1: 0.4}, # customized pruning ratios for layers or blocks\n","    ignored_layers=ignored_layers,\n",")"]},{"cell_type":"markdown","metadata":{"id":"_EP5aJGe3OXT"},"source":["# Pruning and Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCklrMu2ek-3"},"outputs":[],"source":["# pruning + finetuning\n","for i in range(iterative_steps):\n","\n","    # train the model\n","    training_loop(model, trainloader, i)\n","\n","    # evaluate the model accuracy and measurements such as number of parameters, MAC, and Size\n","    print_model_test_report(\"test result before pruning (again)\", model, example_inputs, testloader)\n","\n","    # perform one step of pruning\n","    pruner_iterative.step()\n","\n","    # separate the next iteration of pruning with one line\n","    print(\"\\n\"+ \"=\"*160 + \"\\n\")\n","\n","# evaluate the model after the final iteration of the pruning\n","print_model_test_report(\"final result after \" + str(i + 1) + \" times pruning\", model, example_inputs, testloader)"]},{"cell_type":"markdown","source":["Quantization\n","====\n"],"metadata":{"id":"SQ9AqLr0Hvzn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKc1H4vgeM0D"},"outputs":[],"source":["model_fp32 = model\n","\n","# model must be set to eval for fusion to work\n","model_fp32.eval()\n","\n","# Define the new QConfig with quant_min and quant_max\n","activation_observer = HistogramObserver.with_args(quant_min=0, quant_max=127)\n","weight_observer = PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)\n","qconfig = QConfig(activation=activation_observer, weight=weight_observer)\n","model_fp32.qconfig = qconfig\n","# model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n","\n","\n","# Fuse the activations to preceding layers, where applicable.\n","# This needs to be done manually depending on the model architecture.\n","# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n","model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv1', 'relu1'], ['conv2', 'relu2']])\n","\n","# Prepare the model for QAT. This inserts observers and fake_quants in\n","# the model needs to be set to train for QAT logic to work\n","# the model that will observe weight and activation tensors during calibration.\n","model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused.train())\n","\n","# the right model should be passed to the optimizer\n","optimizer = optim.SGD(model_fp32_prepared.parameters(), lr=0.001, momentum=0.9)\n","\n","\n","for epoch in range(2):\n","\n","    training_loop(model_fp32_prepared.to(device), trainloader, epoch)\n","\n","    # Convert the observed model to a quantized model. This does several things:\n","    # quantizes the weights, computes and stores the scale and bias value to be\n","    # used with each activation tensor, and replaces key operators with quantized\n","    # implementations.\n","    model_int8 = torch.ao.quantization.convert(model_fp32_prepared.to('cpu'))\n","\n","    print_model_test_report(\"quantized model test result after epoch %d\" % (epoch+1), model_int8, example_inputs, testloader, device='cpu')\n","\n","    # separate after quantization with one line\n","    print(\"\\n\"+ \"=\"*160 + \"\\n\")\n","\n","    # if epoch > 3:\n","    #     # Freeze quantizer parameters\n","    #     model_fp32_prepared.apply(torch.ao.quantization.disable_observer)"]},{"cell_type":"markdown","source":["Export to ONNX\n","====\n"],"metadata":{"id":"XUjf_5dm_QD6"}},{"cell_type":"code","source":["PATH = './pruned_qat_mnist.pth'\n","torch.save(model_int8.state_dict(), PATH)\n","\n","model_int8.eval()  # Set the model to inference mode\n","model_int8.to('cpu')\n","\n","dummy_input = torch.randn(1, 1, 28, 28)\n","\n","torch.onnx.export(model_int8,                  # model being run\n","                  dummy_input,            # model input (or a tuple for multiple inputs)\n","                  \"pruned_qat_mnist.onnx\",           # where to save the model\n","                  export_params=True,     # store the trained parameter weights inside the model file\n","                  opset_version=17,       # the ONNX version to export the model to\n","                  do_constant_folding=True,  # whether to execute constant folding for optimization\n","                  input_names=['input'],  # the model's input names\n","                  output_names=['output'],# the model's output names\n","                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes\n","                                'output': {0: 'batch_size'}})"],"metadata":{"id":"N-VYQwN1_J_S"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb","timestamp":1715696210218}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}